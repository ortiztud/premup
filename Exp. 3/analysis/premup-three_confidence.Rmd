---
title: "Analysis of recognition memory as a function of confidence responses on PREMUP-three."
output: html_document
---

Author: Javier Ortiz-Tudela (Goethe Uni)

```{r setup, include=FALSE}
## Load libraries.
library(ggpubr)
library(rstatix)
library(psycho)
library(gridExtra)
library(dplyr)
library(lme4)

## Set directories
if(Sys.info()["sysname"]=="Linux"){
  data_dir <- "/home/javier/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-three/outputs/group_level/share/"
  out_dir <- "/home/javier/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-three/outputs/group_level/" 
}else{
  data_dir <- "/Users/javierortiz/Documents/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-three/outputs/group_level/share/"
  out_dir <- "/Users/javierortiz/Documents/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-three/outputs/group_level/"
}
setwd(out_dir)
## Read-in the data
full_name <- paste(data_dir, "group_task-rec.csv", sep="")

# Since the data is in a CSV file, you can read it with the csvread function.
temp <- read.csv2(full_name, sep=";")

# The data file contains a lot of information that we will not need now.
data <- subset(x = temp,
               select = c("participant", "session", "fillers", "id_acc", "PE_level", "OvsN", "scn_condition", "rec_conf_resp" ))

# In order to simplify further steps we can remove the "new" and filler trials.
data <- data[!is.na(data$OvsN),]
data <- data[data$OvsN==1,]
data <- data[data$fillers==0,]

# Remove bad subjects. See premup-pilot_overall_recognition.html to see how this is calculated
bad_subs <- c(3,13,38,36,39,41,42,43,47,48,52,58,60,61,65,68,69,71,72)
data <- data %>% 
  filter(!participant %in% bad_subs)

```

I will discretize confidence responses into high and low confidence.
```{r}

# Re-code responses
data$rk_resp <- NA
data$rk_resp[data$rec_conf_resp < 3] <- "low"
data$rk_resp[data$rec_conf_resp > 2] <- "high"

# Check numbers
data %>% 
  group_by(participant, session, rk_resp) %>% 
  count(rk_resp) %>% 
  group_by(session, rk_resp) %>% 
  summarise(av_n = mean(n))
```

And now I will look at memory performance by response type

```{r}

## Stats
# First convert the categorical variable into a factor. This is R terminology.
#data$PE_level<-as.factor(data$PE_level)
data$id_acc <- as.numeric(data$id_acc)
data$session <- as.factor(data$session)

# First we can create a plot to visually inspect the pattern of our participants.
ggplot(data, aes(y=id_acc, x = PE_level))+
  geom_bar(stat="summary") + 
  facet_grid(cols = vars(session, rk_resp)) +
  ylab("Hit rate")

# We can check averages
gd <- data %>% 
  group_by(participant, session, rk_resp,PE_level) %>% 
  summarise(id_acc = mean(id_acc), scn_condition = mean(scn_condition))

```

We will run the MLM now.

```{r}
# Run GLMM
data$PE_level <- as.factor(data$PE_level)
# Define linear
linear_comp <-c(-2,-1,0,1,2)

# Define quadratic
quadratic_comp <-c(2,-1,-2,-1,2)

# Some R magic
contrasts(data$PE_level)<-cbind(linear_comp, quadratic_comp)

# Winning from premup-priors_rec_MLM.R
red_model1 <-glmer(id_acc~PE_level + session + (1 | participant), data = data, family = binomial, 
                   control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

# Model with confidence
red_model_conf <-glmer(id_acc~PE_level * session * rk_resp + (1 | participant), data = data, family = binomial, 
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

# Compare models
anova(red_model1, red_model_conf)

# Since we have a significant decrease in model fit, we accept red_model_conf. We can explore this model with "Anova" (capital A)
Anova(red_model_conf)

# Exponentiating the beta estimates gives you the effect size (exp(x)= odds ratio -OR-). We need summary to get the estimate
summary(red_model_conf)
setwd(out_dir)
# Create summary table for the model selection process
source('/home/javier/git_repos/premup/analysis/report_LMM.R')
tested_models <- c(red_model_conf,red_model1)
model_names <- c("Maximal model","Reduced 1")
against_models <- c(1)
report_title <- "Experiment 3. Object recognition (confidence)"
report_table <- report_LMM(tested_models, model_names,against_models,report_title)

```

```{r}
# Box plot
gd$PE_level <- as.factor(gd$PE_level)
sess_lab <- c("Immediate", "Delayed")
names(sess_lab) <- c("1", "2")
ggplot(data = gd,
       aes(x = PE_level, y = id_acc, color = factor(scn_condition))) +
  geom_boxplot() + 
  stat_summary(fun=mean, geom="point", shape=20, size=5, color="black", fill="red") +
  geom_jitter(shape=10, position=position_jitter(0.2)) +
  facet_grid(cols = vars(session, rk_resp), labeller = labeller(session = sess_lab)) + 
  ylab("Hit rates") + 
  ylim(c(0,.8)) +
  xlab("PE strength") +
  theme(legend.position = "top") +
  scale_color_discrete(name = "Context type", labels= c("Flat prior", "Weak prior", "Strong prior"))


```

```{r}
agg_data <- gd %>% 
  group_by(session, rk_resp, PE_level) %>% 
  summarise(id_acc = mean(id_acc))

# Attempts at making spaghetti plot
ggplot(data = gd, 
       aes(x = PE_level, y = id_acc, group = factor(participant), color = factor(participant))) +
  geom_line(stat="smooth", method= "lm", formula = y~ poly(x,2), alpha = 0.5) +
  facet_grid(cols = vars(session, rk_resp)) + 
  ylim(c(0,1))+ 
  ylab("Hit rates") + 
  labs(title = paste("Hit rate by session and confidence"))


```

