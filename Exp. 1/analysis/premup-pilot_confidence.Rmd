---
title: "Analysis of recognition memory as a function of confidence responses on PREMUP-pilot."
output: html_document
---

Author: Javier Ortiz-Tudela (Goethe Uni)

```{r setup, include=FALSE}
## Load libraries.
library(ggpubr)
library(rstatix)
library(psycho)
library(gridExtra)
library(dplyr)
library(lme4)

## Set directories
if(Sys.info()["sysname"]=="Linux"){
  data_dir <- "/home/javier/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-pilot/outputs/group_level/share/"
  out_dir <- "/home/javier/pepe/2_Analysis_Folder/PIVOTAL/PREMUP/premup-pilot/outputs/group_level/" 
}else{
  data_dir <- "/Users/javierortiz/Documents/pepe3/2_Analysis_Folder/PIVOTAL/PREMUP/premup-pilot/outputs/group_level/share/"
  out_dir <- "/Users/javierortiz/Documents/pepe3/2_Analysis_Folder/PIVOTAL/PREMUP/premup-pilot/outputs/group_level/"
}

## Read-in the data
full_name <- paste(data_dir, "group_task-rec.csv", sep="")

# Since the data is in a CSV file, you can read it with the csvread function.
temp <- read.csv2(full_name, sep=";")

# The data file contains a lot of information that we will not need now.
data <- subset(x = temp,
               select = c("participant", "session", "fillers", "id_acc", "PE_level", "OvsN", "scn_condition", "rec_conf_resp" ))

# In order to simplify further steps we can remove the "new" and filler trials.
data <- data[!is.na(data$OvsN),]
data <- data[data$OvsN==1,]
data <- data[data$fillers==0,]

# Remove bad subjects. See premup-pilot_overall_recognition.html to see how this is calculated
poor_performers = c(7, 16, 19, 20, 23)
data <- data %>% 
  filter(!participant %in% poor_performers)

```

I will recode confidence responses into low and high confidence.
```{r}

# Re-code responses
data$rk_resp <- NA
data$rk_resp[data$rec_conf_resp < 4] <- "low"
data$rk_resp[data$rec_conf_resp > 3] <- "high"

# Some confidence responses where missed perhaps due to participants pressing the wrong key are not releasing the previous one in time.
data <- data[!is.na(data$rk_resp),]

# Check numbers
data %>% 
  group_by(participant, session, rk_resp) %>% 
  count(rk_resp) %>% 
  group_by(session, rk_resp) %>% 
  summarise(av_n = mean(n))

```

And now I will look at memory performance by respnse type

```{r}

## Stats
# First convert the categorical variable into a factor. This is R terminology.
data$PE_level<-as.factor(data$PE_level)
data$id_acc <- as.numeric(data$id_acc)
data$session <- as.factor(data$session)

# First we can create a plot to visually inspect the pattern of our participants.
data$PE_plot[data$PE_level == 0.20] <- 1
data$PE_plot[data$PE_level == 0.67] <- 2
data$PE_plot[data$PE_level == 0.80] <- 3
ggplot(data, aes(y=id_acc, x = PE_plot))+
  geom_bar(stat="summary") + 
  facet_grid(cols = vars(session, rk_resp)) +
  ylab("Hit rate")

# We can check averages
gd <- data %>% 
  group_by(participant, session, rk_resp,PE_level) %>% 
  summarise(id_acc = mean(id_acc), scn_condition = mean(scn_condition))

```

We will run the MLM now.

```{r}
# Run GLMM
# Define linear
linear_comp <-c(-1,0,1)

# Define quadratic
quadratic_comp <-c(1,-2,1)

# Some R magic
contrasts(data$PE_level)<-cbind(linear_comp, quadratic_comp)

# I'm going to take the winning model from the id_acc standard analysis and add confidence. Then I can compare that model with the original to see if it adds anything.
# Reduced model (-2b). Winning from premup-pilot_rec_MLM.R
red_model2b <-glmer(id_acc~PE_level * session + (session | participant), data = data, family = binomial, 
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

# Model with confidence
red_model_conf <-glmer(id_acc~PE_level * session * rk_resp + (session | participant), data = data, family = binomial, 
                    control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=100000)))

# Compare models
anova(red_model2b, red_model_conf)

# Since we have a significant decrease in model fit, we accept red_model_conf. We can explore this model with "Anova" (capital A)
Anova(red_model_conf)

# Exponentiating the beta estimates gives you the effect size (exp(x)= odds ratio -OR-). We need summary to get the estimate
summary(red_model_conf)
```

```{r}
setwd(out_dir)
# Create summary table for the model selection process
source('/home/javier/git_repos/premup/analysis/report_LMM.R')
tested_models <- c(red_model_conf,red_model2b)
model_names <- c("Reduced 2b_confidence","Reduced 2b")
against_models <- c(1)
report_title <- "Experiment 1. Object recognition (confidence)"
report_table <- report_LMM(tested_models, model_names,against_models, report_title)
```

```{r}
# Box plot
gd$PE_level <- as.factor(gd$PE_level)
sess_lab <- c("Immediate", "Delayed")
names(sess_lab) <- c("1", "2")
ggplot(data = gd,
       aes(x = PE_level, y = id_acc, color = factor(scn_condition))) +
  geom_boxplot() + 
   stat_summary(fun=mean, geom="point", shape=20, size=5, color="black", fill="red") +
  geom_jitter(shape=10, position=position_jitter(0.2)) +
  facet_grid(cols = vars(session, rk_resp), labeller = labeller(session = sess_lab)) + 
  ylab("Hit rates") + 
  ylim(c(0,.8)) +
  xlab("PE strength") +
  theme(legend.position = "top") +
  scale_color_discrete(name = "Context type", labels= c("Flat prior", "Strong prior"))


```

```{r}
agg_data <- gd %>% 
  group_by(session, rk_resp, PE_level) %>% 
  summarise(id_acc = mean(id_acc))

# Attempts at making spaghetti plot
ggplot(data = gd, 
       aes(x = PE_level, y = id_acc, group = factor(participant), color = factor(rk_resp))) +
  geom_line(method= "lm", formula = y~ poly(x,2), alpha = 0.5) +
  facet_grid(cols = vars(session, rk_resp)) + 
  ylim(c(0,1))+ 
  ylab("Hit rates") + 
  labs(title = paste("Hit rate by session and confidence"))


```

